# PROJECT: Dá»° BÃO KHÃCH HÃ€NG Rá»œI Bá» (RETAIL ACTIVE CHURN PREDICTION)

Dá»± Ã¡n Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t Khai phÃ¡ dá»¯ liá»‡u (Data Mining) Ä‘á»ƒ phÃ¢n tÃ­ch hÃ nh vi mua sáº¯m vÃ  dá»± bÃ¡o nguy cÆ¡ rá»i bá» cá»§a nhÃ³m khÃ¡ch hÃ ng "Active" (KhÃ¡ch hÃ ng thÆ°á»ng xuyÃªn) trong lÄ©nh vá»±c bÃ¡n láº».

## ğŸ“‹ Tá»•ng quan vá» PhÆ°Æ¡ng phÃ¡p

* **BÃ i toÃ¡n:** Classification (PhÃ¢n loáº¡i Nhá»‹ phÃ¢n: Rá»i bá» vs á» láº¡i).
* **Äá»‘i tÆ°á»£ng dá»± bÃ¡o:** KhÃ¡ch hÃ ng "Active" (CÃ³ táº§n suáº¥t mua hÃ ng `Frequency >= 2` trong 12 thÃ¡ng qua).
* **Ká»¹ thuáº­t xá»­ lÃ½ dá»¯ liá»‡u:** Rolling Window (Cá»­a sá»• trÆ°á»£t 12 thÃ¡ng) Ä‘á»ƒ tÃ­nh toÃ¡n chá»‰ sá»‘ hÃ nh vi (RFM) thay vÃ¬ tÃ­ch lÅ©y toÃ n bá»™ lá»‹ch sá»­.
* **ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh:** Sá»­ dá»¥ng K-Fold Cross Validation (5-Folds) Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ tin cáº­y.

---

## âš™ï¸ YÃªu cáº§u cÃ i Ä‘áº·t

TrÆ°á»›c khi cháº¡y chÆ°Æ¡ng trÃ¬nh, vui lÃ²ng cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n Python cáº§n thiáº¿t:

    pip install pandas numpy matplotlib seaborn scikit-learn xgboost squarify openpyxl

---

## ğŸ“‚ Cáº¥u trÃºc Source Code

MÃ£ nguá»“n Ä‘Æ°á»£c tá»• chá»©c theo quy trÃ¬nh chuáº©n cá»§a Data Mining, tá»« xá»­ lÃ½ dá»¯ liá»‡u Ä‘áº¿n Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh.

### 1. Giai Ä‘oáº¡n Tiá»n xá»­ lÃ½ & Chuáº©n bá»‹ dá»¯ liá»‡u (Data Preparation)
CÃ¡c script nÃ y chá»‹u trÃ¡ch nhiá»‡m biáº¿n Ä‘á»•i dá»¯ liá»‡u giao dá»‹ch thÃ´ thÃ nh dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao Ä‘á»ƒ huáº¥n luyá»‡n.

* **`create_mart.py`**
    * *Chá»©c nÄƒng:* Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u cáº¥p Ä‘á»™ hÃ³a Ä‘Æ¡n (`sales_bill.csv`) sang dá»¯ liá»‡u cáº¥p Ä‘á»™ khÃ¡ch hÃ ng cÆ¡ báº£n (`sales_mart.csv`).
    * *Xá»­ lÃ½:* LÃ m sáº¡ch dá»¯ liá»‡u, xá»­ lÃ½ giÃ¡ trá»‹ Ã¢m/null.
* **`create_training_data.py`**
    * *Chá»©c nÄƒng:* Táº¡o bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n cuá»‘i cÃ¹ng (`rfm_training_data_mall.csv`) dÃ¹ng cho cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i.
    * *Logic Ã¡p dá»¥ng:*
        * **Rolling Window:** Chá»‰ láº¥y dá»¯ liá»‡u hÃ nh vi trong 12 thÃ¡ng trÆ°á»›c ngÃ y Snapshot (08/12/2022).
        * **Active Filter:** Loáº¡i bá» khÃ¡ch hÃ ng vÃ£ng lai (Frequency = 1).
        * **Labeling:** GÃ¡n nhÃ£n rá»i bá» dá»±a trÃªn hÃ nh vi trong 3 thÃ¡ng cuá»‘i cÃ¹ng (Ä‘áº¿n 08/03/2023).
* **`quantile_segment.py`**
    * *Chá»©c nÄƒng:* PhÃ¢n khÃºc khÃ¡ch hÃ ng thÃ nh 11 nhÃ³m hÃ nh vi (Champions, At Risk, Lost...) dá»±a trÃªn Ä‘iá»ƒm sá»‘ RFM (dÃ¹ng Ä‘á»ƒ bÃ¡o cÃ¡o Insight).

### 2. Giai Ä‘oáº¡n Tinh chá»‰nh tham sá»‘ (Hyperparameter Tuning)
Sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t tÃ¬m kiáº¿m tham sá»‘ tá»‘i Æ°u trÆ°á»›c khi Ä‘Æ°a vÃ o huáº¥n luyá»‡n chÃ­nh thá»©c.

* **`tune_random_forest.py`**: Sá»­ dá»¥ng **Grid Search** Ä‘á»ƒ tÃ¬m tá»• há»£p tá»‘i Æ°u cho `n_estimators`, `max_depth`, `max_features`, `min_samples_leaf`.
* **`tune_XGboost.py`**: Sá»­ dá»¥ng **Grid Search** Ä‘á»ƒ tÃ¬m `learning_rate`, `gamma`, `colsample_bytree`, `max_depth` tá»‘i Æ°u.
* **`tune_knn.py`**: Sá»­ dá»¥ng vÃ²ng láº·p kiá»ƒm thá»­ Ä‘á»ƒ tÃ¬m sá»‘ lÆ°á»£ng hÃ ng xÃ³m **`K`** cÃ³ F1-Score cao nháº¥t (Elbow Method).
* **`diagnose_all_models.py`**: Cháº¡y cháº©n Ä‘oÃ¡n nhanh Ä‘á»™ sÃ¢u cÃ¢y (max_depth) cho cáº£ 3 mÃ´ hÃ¬nh Tree-based Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ xu hÆ°á»›ng.

### 3. Giai Ä‘oáº¡n Huáº¥n luyá»‡n & ÄÃ¡nh giÃ¡ (Modeling & Evaluation)
Cháº¡y cÃ¡c mÃ´ hÃ¬nh vá»›i tham sá»‘ Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u, sá»­ dá»¥ng ká»¹ thuáº­t **K-Fold Cross Validation (5-Folds)**. Má»—i file sáº½ xuáº¥t ra Ma tráº­n nháº§m láº«n vÃ  BÃ¡o cÃ¡o phÃ¢n loáº¡i chi tiáº¿t.

* **`decision_tree.py`**: Cháº¡y mÃ´ hÃ¬nh CÃ¢y quyáº¿t Ä‘á»‹nh (Decision Tree).
* **`random_forest.py`**: Cháº¡y mÃ´ hÃ¬nh Random Forest (Ensemble Bagging).
* **`XGboost.py`**: Cháº¡y mÃ´ hÃ¬nh XGBoost (Ensemble Boosting) - *MÃ´ hÃ¬nh chiáº¿n tháº¯ng*.
* **`knn.py`**: Cháº¡y mÃ´ hÃ¬nh K-Nearest Neighbors (Distance-based).

### 4. Tá»•ng há»£p & Trá»±c quan hÃ³a (Visualization)
* **`model_comparision.py`**:
    * Tá»•ng há»£p káº¿t quáº£ tá»« 4 mÃ´ hÃ¬nh trÃªn.
    * Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh trá»±c quan cÃ¡c chá»‰ sá»‘: Accuracy, Precision, Recall, F1-Score.
* **`visualize_insight.py`**:
    * Váº½ biá»ƒu Ä‘á»“ **Feature Importance** (Má»©c Ä‘á»™ quan trá»ng cá»§a biáº¿n).
    * Táº¡o 2 phiÃªn báº£n: ToÃ n cáº£nh (tháº¥y rÃµ sá»± Ã¡p Ä‘áº£o cá»§a Frequency) vÃ  Zoom-in (tháº¥y rÃµ cÃ¡c yáº¿u tá»‘ tiá»m áº©n khÃ¡c).

---

## ğŸš€ HÆ°á»›ng dáº«n cháº¡y chÆ°Æ¡ng trÃ¬nh (Step-by-Step)

Vui lÃ²ng cháº¡y theo thá»© tá»± sau Ä‘á»ƒ Ä‘áº£m báº£o luá»“ng dá»¯ liá»‡u chÃ­nh xÃ¡c:

**BÆ°á»›c 1: Táº¡o dá»¯ liá»‡u**
    python create_mart.py
    python create_training_data.py

*(Káº¿t quáº£: File `rfm_training_data_mall.csv` sáº½ Ä‘Æ°á»£c táº¡o ra)*

**BÆ°á»›c 2: Cháº¡y Tuning Ä‘á»ƒ tÃ¬m tham sá»‘ má»›i**
    python tune_random_forest.py
    python tune_XGboost.py
    python tune_knn.py

**BÆ°á»›c 3: Cháº¡y cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i (K-Fold)**
    python XGboost.py
    python random_forest.py
    python decision_tree.py
    python knn.py

**BÆ°á»›c 4: Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh vÃ  Insight**
    python model_comparision.py
    python visualize_insight.py

---

## ğŸ“Š Káº¿t quáº£ Ä‘áº§u ra (Artifacts)

Sau khi cháº¡y xong, chÆ°Æ¡ng trÃ¬nh sáº½ sinh ra cÃ¡c file hÃ¬nh áº£nh bÃ¡o cÃ¡o (`.png`) trong thÆ° má»¥c hiá»‡n táº¡i:

1.  **Ma tráº­n nháº§m láº«n (Confusion Matrix) - K-Fold Average:**
    * `xgb_kfold_matrix_final.png`
    * `rf_kfold_matrix_final.png`
    * `dt_kfold_matrix_final.png`
    * `knn_kfold_matrix_final.png`

2.  **PhÃ¢n tÃ­ch nhÃ¢n tá»‘ áº£nh hÆ°á»Ÿng (Feature Importance):**
    * `xgb_feature_importance_final.png`
    * `rf_feature_importance_final.png`
    * `insight_xgboost.png` (Bao gá»“m cáº£ báº£n Zoom-in vÃ  ToÃ n cáº£nh)

3.  **Biá»ƒu Ä‘á»“ Tá»‘i Æ°u hÃ³a tham sá»‘:**
    * `xgb_tuning_chart.png`
    * `rf_tuning_chart.png`
    * `knn_tuning_chart.png`
    * `dt_tuning_chart.png`

4.  **Tá»•ng há»£p:**
    * `model_comparison.png` (So sÃ¡nh hiá»‡u nÄƒng 4 mÃ´ hÃ¬nh).
